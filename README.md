# LCV Retail Analytics Platform

A comprehensive **end-to-end data warehouse and analytics platform** designed to simulate LC Waikiki's (LCV) global retail operations. This project demonstrates advanced data engineering, analytics, and machine learning skills aligned with enterprise-scale data platforms.

---

## ğŸ¯ Project Overview

**Objective:** Build a production-grade analytics platform that transforms raw transactional data from retail sources into actionable business intelligence and predictive insights.

**Context:** LC Waikiki operates 1,300+ stores across 60+ countries. This project simulates their data flow and builds the kind of analytics infrastructure they rely on for decision-making.

**Scope:**
- **Source System**: Mock Point-of-Sale (POS) data in PostgreSQL
- **Data Warehouse**: Google BigQuery with star schema
- **Transformations**: dbt for ETL/ELT pipelines
- **Analytics**: SQL views for KPI calculations
- **BI Dashboard**: Looker Studio for visualization
- **ML Model**: Churn prediction + Demand forecasting API
- **Governance**: Data dictionary, lineage, quality checks

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PostgreSQL (Source)                       â”‚
â”‚  FactSales | DimProduct | DimStore | DimDate | DimCustomer  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Extract
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Google Cloud Storage (GCS - Staging)             â”‚
â”‚            Parquet files (daily batches)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Load
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Google BigQuery (Data Warehouse)                     â”‚
â”‚  Raw Layer    â”‚    Staging Layer    â”‚    Mart Layer          â”‚
â”‚  (raw_sales)  â”‚ (stg_sales_clean)   â”‚ (fct_sales_v2)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Transform (dbt)
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Analytic Views (Consumption Layer)               â”‚
â”‚  YoY Growth | RFM Scores | Churn Risk | Forecasts           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â–¼          â–¼          â–¼
    [BI]      [ML API]    [Exports]
  Looker    Churn/Demand   Reports
  Studio    Predictions
```

---

## ğŸ“Š Key Features

### 1. **Data Modeling** (Week 1)
- **Star Schema**: FactSales + Dimension tables (Product, Store, Date, Customer)
- **Slowly Changing Dimensions (SCD)**: Handle product price/store location changes
- **Normalized OLTP** + **Denormalized Analytics** structures
- Full ERD documentation

### 2. **Advanced SQL** (Week 1â€“2)
- Year-over-Year (YoY) sales growth analysis
- Customer segmentation via RFM (Recency, Frequency, Monetary)
- Inventory turnover by product category
- Churn detection (inactive customers)
- Window functions, CTEs, query optimization

### 3. **ETL/ELT Pipeline** (Week 2â€“3)
- Extract data from Postgres â†’ Cloud Storage (Parquet)
- Load into BigQuery raw layer (daily ingestion)
- dbt transformations: staging â†’ marts
- Data quality checks (duplicates, nulls, referential integrity)

### 4. **Performance & Cost Optimization** (Week 2â€“3)
- Partition FactSales by `sale_date` (reduce scanned bytes)
- Clustering by `store_id` + `product_id` (faster aggregations)
- Query cost benchmarking and improvements
- Documentation of optimization wins

### 5. **BI Dashboard** (Week 4)
- **Sales Performance**: Total revenue, trend analysis, regional breakdown
- **Customer Metrics**: Acquisition, retention, lifetime value (CLV)
- **Inventory Insights**: Turnover rates, stockouts, reorder levels
- **KPI Tracking**: Define business metrics (e.g., "Sell-through Rate", "Inventory Days")

### 6. **Churn Prediction** (Week 5)
- **Behavioral Churn**: Flag inactive customers (no purchase in 90+ days)
- **ML Model**: Logistic Regression / Random Forest to predict churn probability
- **FastAPI Endpoint**: `/churn-risk?customer_id=123` for real-time predictions
- Feature engineering: RFM, purchase trend, loyalty program status

### 7. **Demand Forecasting** (Week 5)
- Predict next month's sales by product/store
- Time-series or ensemble model (ARIMA / Prophet / XGBoost)
- API endpoint for integration with planning systems

---

## ğŸ“ Project Structure

```
lcv-retail-analytics-platform/
â”œâ”€â”€ .gitignore                      # Git ignore rules
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ ROADMAP.md                      # 5-week detailed plan
â”œâ”€â”€ GOVERNANCE.md                   # Data dictionary & lineage
â”‚
â”œâ”€â”€ SCHEMA/
â”‚   â”œâ”€â”€ star_schema.sql             # Star schema DDL
â”‚   â”œâ”€â”€ erd_diagram.png             # Entity-Relationship Diagram
â”‚   â””â”€â”€ data_dictionary.md          # Column definitions & lineage
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ postgres/
â”‚   â”‚   â”œâ”€â”€ init.sql                # Schema setup
â”‚   â”‚   â””â”€â”€ seed_synthetic_data.py  # Generate test data
â”‚   â”‚
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ postgres_to_gcs.py      # Extract & load to GCS
â”‚   â”‚   â”œâ”€â”€ dbt_project/            # dbt models (transformations)
â”‚   â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ staging/        # Raw â†’ Clean
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ marts/          # Analytics-ready tables
â”‚   â”‚   â”‚   â””â”€â”€ dbt_project.yml
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â”œâ”€â”€ views.sql               # KPI views & analysis queries
â”‚   â”‚   â””â”€â”€ kpi_definitions.md      # Business logic for each KPI
â”‚   â”‚
â”‚   â””â”€â”€ ml/
â”‚       â”œâ”€â”€ churn_model.py          # Churn prediction model
â”‚       â”œâ”€â”€ demand_forecast.py      # Demand forecasting model
â”‚       â”œâ”€â”€ api.py                  # FastAPI endpoints
â”‚       â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ dashboards/
â”‚   â”œâ”€â”€ looker_studio_config.md     # Dashboard setup guide
â”‚   â””â”€â”€ queries.sql                 # Pre-built dashboard queries
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ data_quality_checks.py      # dbt tests + custom checks
â”‚   â””â”€â”€ test_ml_models.py           # Model performance tests
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ ARCHITECTURE.md             # System design decisions
    â””â”€â”€ PERFORMANCE_LOG.md          # Query optimization results
```

---

## ğŸš€ Quick Start

### Prerequisites
- PostgreSQL 12+
- Python 3.9+
- Google Cloud account (free tier for BigQuery)
- Git

### Local Setup

```bash
# Clone and navigate
cd lcv-retail-analytics-platform

# Create virtual environment
python -m venv venv
source venv/Scripts/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Setup PostgreSQL
psql -U postgres -f SCHEMA/star_schema.sql

# Generate synthetic data
python src/postgres/seed_synthetic_data.py

# Run ETL pipeline (local testing)
python src/etl/postgres_to_gcs.py --local

# Run dbt transformations
cd src/etl/dbt_project
dbt run
dbt test
```

### BigQuery Setup

```bash
# Create BigQuery dataset
bq mk --dataset --location=US retail_analytics

# Load sample data
bq load --source_format=PARQUET retail_analytics.raw_sales gs://your-bucket/sales/*.parquet

# Transform with dbt
dbt run --profiles-dir .
```

---

## ğŸ“ˆ Skills Demonstrated

| Requirement (LCW Job Description) | How This Project Shows It |
|---|---|
| **Advanced SQL** | Complex queries: window functions, CTEs, performance tuning with indexes |
| **Data Modeling** | Star schema + normalized OLTP; conceptual â†’ logical â†’ physical models |
| **Cloud Platforms** | BigQuery, Cloud Storage, dbt; partitioning & clustering for cost optimization |
| **ETL/ELT Pipelines** | Postgres â†’ GCS â†’ BigQuery; dbt transformations; daily automation |
| **Analytics Products** | Looker Studio dashboards; clear KPI definitions; business-focused insights |
| **Data Governance** | Data dictionary, lineage tracking, quality checks, SCD handling |
| **AI Integration** | Churn prediction + demand forecasting APIs; feature engineering for ML |

---

## ğŸ“Š Datasets

**Synthetic Data Scope** (for demo):
- **Stores**: 50 locations (1â€“50 store_id)
- **Products**: 500 products (categories: textile, accessories, footwear)
- **Customers**: 10,000 unique customers
- **Time Range**: 24 months of transaction history
- **Transaction Volume**: ~1M sales records

This is enough to demonstrate performance optimization and pattern analysis without needing real LCV data.

---

## ğŸ” Data Governance

- **Data Dictionary**: Column definitions, data types, and business logic ([GOVERNANCE.md](GOVERNANCE.md))
- **Lineage**: Source â†’ Staging â†’ Marts tracking in dbt
- **Quality Checks**: dbt tests + custom Python validations
- **SCD Handling**: Type 2 (track history) for product prices and store attributes

---

## ğŸ“ Learning Outcomes

After completing this project, you will have hands-on experience with:
- âœ… Enterprise data warehouse design
- âœ… Cloud data platform development (GCP)
- âœ… SQL optimization for cost and performance
- âœ… ETL/ELT pipeline orchestration
- âœ… BI and dashboard design
- âœ… End-to-end ML model deployment
- âœ… Data governance and documentation

---

## ğŸ“… Timeline

- **Week 1**: Data modeling + SQL (5 working days)
- **Week 2**: ETL pipeline + transformations (5 working days)
- **Week 3**: BigQuery + performance optimization (5 working days)
- **Week 4**: BI dashboards + KPIs (5 working days)
- **Week 5**: ML models + API + documentation (5 working days)

**Effort**: ~3 hours/day = **~75 hours total**

---

## ğŸ”— Documentation

- [ROADMAP.md](ROADMAP.md) â€” Detailed week-by-week plan
- [GOVERNANCE.md](GOVERNANCE.md) â€” Data dictionary & lineage
- [SCHEMA/](SCHEMA/) â€” Data modeling details
- [src/analytics/kpi_definitions.md](src/analytics/kpi_definitions.md) â€” Business metric definitions

---

## ğŸ‘¤ Author

Built as a portfolio project to demonstrate data engineering + analytics skills for LC Waikiki (LCW) opportunities.

---

## ğŸ“ License

MIT

---

**Status**: ğŸš€ In Progress (Week 1 starting soon)

*Last Updated: February 14, 2026*
